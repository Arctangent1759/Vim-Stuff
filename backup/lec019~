Asymptotic Analysis
===================
	Inventory:
		10,000ms to read inventory from disk.
		10ms to process each transaction.
		n transactoins take (10,000 + 10n)ms.

		The 10ms is more important because it becomes huge as n gets large.
	
Big-Oh Notation (upper bounds)
==============================
	Let n be the size of a program's input.
	Let T(n) be a function.
	Let f(n) be another function-preferably simple.

	T(n) \in O(f(n)) if and only if T(n) \le cf(n), c \in \reals whenever n is big, for some large constant c, 

	How big is "big"?

	How big is "big"? Big enough to make T(n) fit under cf(n).

	How large is c? Big enough to make T(n) fit under cf(n).

	Example: Inventory

		Inventory runs in O(n)

		T(n) = 10,000+10n

		Try out f(n)=n

		Pick c=20

		For all n \ge 100, T(n) \le cf(n)

		Therefore, T(n) \in O(n)

	Formally, O(f(n)) is the set of all functions that satisfy:

		\_There exist positive constants c and N such that for all n \ge N, T(n) \le cf(n).\_

	Example:
		1. 1,000,000n \in O(n). Proof: set c=1,000,000; N=0. Big Oh notation doesn't care about constant factors.
			Unnecessary to write O(2n) because O(2n)=O(n)

		2. n \in O(n^3) Proof: set c=1; N=1
			Time \in O(n^3) doesn't mean it's slow.
			Big-Oh only gives you the Upper Bound.

		3. n^3+n^2+n \in O(n^3)

Table of important Big-Oh Sets
==============================
	Smallest to largest:
	Table:
		Function		Common Name
		--------		-----------
		O(n)			constant
		O(log(n))		logarithmic
		O(log^2(n))		log-squared
		O(\sqrt(n))		root-n
		O(n)			linear
		O(n log(n))		<no name>
		O(n^2)			quadratic
		O(n^3)			cubic
		O(n^4)			quartic
		O(2^n)			exponential
		O(e^n)			exponential
		O(n!)			factorial
	Notice that exponential times aren't asymptotically identical

	An \*efficient algorithm\* runs on O(n log(n)) or less.
	An \*useless algorithm\* runs on n^7 or less.

Warnings
========
	1. Fallacious proof:
		n^2 \in O(n) Proof: Choose c=n, then n^2 \le n^2
		Wrong! c must be a constant, not dependent on n

	2. Big-Oh notation does not say what the functions mean.
		Binary search on an array:
			-Worst case running time \in O(log(n)).
			-Best case running time \in O(1).
			-Memory use \in O(n)
			-47+18 log(n)-3/n \in O(the worst case running time)
	3. Sometimes constant factors matter:
		"e^(3n) \in O(e^n) because constant factors don't matter"
		e^(3n)/e^n=e^2n, a fast growing constant.
	4. Big-Oh notation doesn't tell the whole story:
		T(n)=n log_2 n ms
		U(n)=100 ms

		If n is big enough, U(n) is better. But not for any n smaller than the number of particles in the niverse.

		T(n) dominates U(n) asymptotically, but log_2 n < 50 in practice.
