gcc -ggdb -Wall -pedantic -std=c99 -D_GNU_SOURCE -O3    matrixMultiply.c   -o matrixMultiply
gcc -o matrixMultiply -ggdb -Wall -pedantic -std=c99 -D_GNU_SOURCE -O3 matrixMultiply.c
./matrixMultiply
Running Part A...

n = 10, 1.000 Gflop/s
n = 14, 1.372 Gflop/s
n = 19, 1.524 Gflop/s
n = 26, 1.598 Gflop/s
n = 35, 1.588 Gflop/s
n = 47, 1.573 Gflop/s
n = 63, 1.558 Gflop/s
n = 85, 1.473 Gflop/s
n = 114, 1.497 Gflop/s
n = 153, 1.522 Gflop/s
n = 205, 1.088 Gflop/s
n = 274, 1.516 Gflop/s
n = 366, 1.422 Gflop/s
n = 489, 1.254 Gflop/s
n = 653, 0.888 Gflop/s
n = 871, 0.227 Gflop/s
n = 1000, 0.200 Gflop/s


./matrixMultiply 2
Running Part B...

ijk:    n = 1000, 0.195 Gflop/s
ikj:    n = 1000, 0.090 Gflop/s
jik:    n = 1000, 0.197 Gflop/s
jki:    n = 1000, 1.185 Gflop/s
kij:    n = 1000, 0.090 Gflop/s
kji:    n = 1000, 1.520 Gflop/s




1. Think about the naive matrix multiplication algorithm. Is it relatively good or bad in terms of locality? Why?

  The algorithm is relatively bad in terms of locality. It acts on entries in each array with a step of n, rather than iterating sequentially.

2. Why does performance drop for large values of n? (Hint: what gets loaded on a cache miss? How long does that data stay there?)

  As n increases, the step size increases, decreasing spatial locality. If n is a significant portion of the block size, the number of hits per iteration decreases, since the portion of the block being used decreases.
  
3. Which of the 6 different loop orderings perform best for 1000-by-1000 matrices? Which ordering(s) perform the worst?
kji is the best.
kij and ijk are the worst.
  
4. How does the way we stride through the matrices with respect to the innermost loop affect performance?
In kji, the loop iterates through the indices sequentially. This means that it makes maximal use of the blocks in the cache for two of the arrays.
