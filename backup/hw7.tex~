% Search for all the places that say "PUT SOMETHING HERE".

\documentclass[10pt]{article}
\usepackage{amsmath,textcomp,amssymb,graphicx}
\usepackage{tikz,pgfplots}
\usetikzlibrary{arrows}

\def\Name{Alexander Chu}  % Your name
\def\Sec{107, Yun Park}  % Your GSI's name and discussion section
\def\Login{cs170-ix} % Your login
\def\Homework{7}%Number of Homework, PUT SOMETHING HERE
\def\Session{Fall 2013}


\title{CS170--Fall 2013 --- Solutions to Homework 7\vspace{-2ex}}
\author{\Name, section \Sec, \texttt{\Login}}
\markboth{CS170--\Session\  Homework \Homework\ \Name, section \Sec}{CS170--\Session\ Homework \Homework\ \Name, section \Sec, \texttt{\Login}}
\pagestyle{myheadings}

\usepackage[margin=0.125in]{geometry}

\begin{document}
\maketitle
\vspace{-4ex}
\textbf{Collaborators}: Robert Chang, Rohan Chitnis, Jong Ahn, Leo Wu
\vspace{-4ex}
\section*{Problem 1}
\textbf{Main Idea:}\\
We know that any node  that is not a super-schmoozer has a maximum of 19 edges, so it is possible to iterate through all neighbors of these nodes in constant time.
We take advantage of this by running successive updates on non-super-schmoozer's neighbors.
First, use depth-first-search to find the degree of each vertex.
Then, find all vertices that have a degree less than 20, and add them to queue. 
From now, this queue mantains the invariant that all elements added have had at least one neighbor removed.
While the queue is not empty, pop an element off the queue and check if its degree is less than 20. If so, mark it for removal, decrement the degrees of each of its children, and add each child to the queue if it is not already in the queue.
When the queue is empty, the graph should contain only super-schmoozer nodes.
Now, run the connected components algorithm for undirected graphs on the remaining nodes, and return the largest component.\\
\textbf{Pseudocode:}
\begin{verbatim}
def super_schmoozer(G):
    #Input: Graph G with vertices V and E
    (V,E)=G
    degree=find_degrees_DFS(G)
    enQueued = Empty_Set()
    marked_for_removal = Empty_Set()
    queue = makeQueue([v for v in V if degree(v)<20])
    while not isEmpty(queue):
        curr=queue.dequeue()
        if degree(curr)<20:
            marked_for_removal.add(curr)
            for each neighbor n of curr:
                degree(n)-=1
                if not enQueued(n):
                    enQueued(curr)=True
                    queue.enqueue(n)
    for v in marked_for_removal:
        G.remove(v)
    return get_largest_connected_component(G)
\end{verbatim}
\textbf{Proof of Correctness:}\\
We know that potential non-super-schmoozer nodes are either nodes that have degrees of less than 20, or are neighbors of non-super-schmoozer nodes.
Hence, to show that a node is not a super-schmoozer, we first must show that one of its neighbors is a not a super-schmoozer.
Our algorithm enforces this; in order for a node to lose its super-schmoozer status by being removed from the graph, it must have at some point entered the queue.
But a node can only enter a queue if one of its parents is marked for removal. 
Since the space of all possible non-super-schmoozers consists of neighbors of confirmed non-super-schmoozers, the algorithm tests each node that could possibly be a non-super-schmoozer, and removes it if its vertex count falls below a threshold.
Hence, after running non-super-schmoozer removal, we are left with only networks of super-schmoozers.
Running the undirected connected components algorithm on the graph leaves us with sets of disconnected super-schmoozers.
Returning the largest component thus yields the correct solution.\\
\textbf{Running Time:}\\
$\boxed{O(|V|+|E|)}$\\
\textbf{Running Time Analysis:}\\
The algorithm consists of an initial depth-first search, non-super-schmoozer removal, and getting the largest strongly connected component. 
The first and last both take $O(|V|+|E|)$ time.
In the non-super-schmoozer removal algorithm, at the worst case, each vertex is added to the queue exactly once, since it will not be added again once its enQueued flag has been set.
Since vertices that must be removed have a limit of 20 edges, the marking for removal process takes $O(20|V|)$ time.
An additional $O(|V|)$ time is necessary to remove the marked nodes from the graph.
Hence, the total running time is $O(23|V|+2|E|)=\boxed{O(|V|+|E|)}$ time.

\label{pg:end-of-p1}

% Make sure that the solution here does not exceed one page here. If
% it does, use the extra space for this problem at the end.  
%
% Comment out the next line if you are NOT using the extra space
\paragraph{} \emph{Continued on Page \pageref{pg:p1-continuation}}
\newpage


%%Do NOT remove/comment the next line
\pagestyle{plain}


%%It makes sure your name appears only on the first page
\section*{Problem 2}
\subsection*{Part (a)}
Since each vertex has has at most 51 neighbors, and is only considered bad if it conflicts with 25 of them, there is always gaurunteed to be a setting in which the vertex is good when compared to its neighbors. 
That is, if a good vertex is flipped, then it becomes bad. If a bad vertex is flipped, it becomes temporarily good.
We aim to prove that the number of flips on a given vertex is finite. 
Once a vertex is flipped, it becomes good, and stays good unless 25 of its neighbors take on its color.
But the same rule applies to each of the neighbors; once flipped, these vertices stay good until their neighbors flip to take on their colors.
If we model the graph as a BFS tree, with a given vertex v as the root, we observe that the maximal number of times that v can change is bounded on the number of descendents that v has in the BFS tree.
To show this, we observe that when v is flipped, it cannot flip again until it becomes bad again, which necessitates that 25 of its children are v's color.
In the worst case, v has 51 children, 25 of which are blue and 26 of which are gold. In this case, thrashing can occur, as children flip between blue and gold such that v becomes bad each time.
Each of these children flip a number of times equal to their childrens' flips.
Hence, v changes as changes from its bfs tree propogate up. Hence, v changes a maximum number of times equal to the number of descendents it has in its BFS tree.
Hence, the algorithm is gaurunteed to terminate.

\subsection*{Part (b)}
We have shown before that each vertex v can flip a number of times equal to the depth of the BFS tree of the graph starting from v.
But if the graph is connected, the worst case number of flips is equal to V, the number of nodes.
Hence, the maximum number of iterations is in $O(|V|^2)$. But $|V|^2 \in O(|E|)$

\label{pg:end-of-p2}

%Insert solution here

% Make sure that the solution here does not exceed one page here. If
% it does, use the extra space for this problem at the end.  
%
% Comment out the next line if you are NOT using the extra space
\paragraph{} \emph{Continued on Page \pageref{pg:p2-continuation}}



\newpage

\section*{Problem 3}
\subsection*{Part (a)}
The algorithm is suboptimal. Consider the following set of tasks:\\
$
[(d_0=0, p_0=0),
(d_1=1, p_1=200),
(d_2=1, p_2=100)]
$\\
The algorithm will schedule task 0 to time 0, task 1 to time 1, and task 2 to time 2, resulting in a penalty of 100.
The optimal solution will schedule task 1 to time 0, task 2 to time 1, and task 0 to time 2, resulting in a penalty of 0.
Hence, the algorithm is not optimal.
\subsection*{Part (b)}
The algorithm is suboptimal. Consider the following set of tasks:\\
$
[(d_0=2, p_0=2),
(d_1=0, p_1=1),
(d_2=1, p_2=1)]
$\\
The algorithm will schedule task 0 to time 0, task 2 to time 1 (since task 1 can no longer be completed on time), and task 1 to time 2, resulting in a penalty of 1.
The optimal solution will schedule task 1 to time 0, task 2 to time 1, and task 0 to time 2, resulting in a penalty of 0.
Hence, the algorithm is not optimal.
\subsection*{Part (c)}
The algorithm is optimal.\\
Suppose that each task (d,p) in the set of tasks has a distinct deadline $d_i\neq d_j, i\neq j$. 
Then all tasks can be scheduled on time on the day of its deadline $d_i$, and thus, the optimal penalty is 0.
In this case, the algorithm produces the optimal solution by scheduling each task at the latest date before its deadline $d_i$.
The optimal penality is only nonzero when there are tasks with deadlines on the same day, and when deadlines are packed close enough together such that some task cannot be completed on time.
Suppose there are task i with $(d_i,p_i)$ and task j with $(d_j,p_j)$, where $d_i=d_j, p_i>p_j$. Suppose also that all timeslots $d_k, d_k<d_i$ are filled.
The optimal choice of scheduling would be to schedule task i first at time $d_i$, since its penality is higher. 
Hence, by ordering by penality, the algorithm produces optimal solutions.
\label{pg:end-of-p3}

% Make sure that the solution here does not exceed one page here. If
% it does, use the extra space for this problem at the end.  
%
% Comment out the next line if you are NOT using the extra space
\paragraph{} \emph{Continued on Page \pageref{pg:p3-continuation}}




\newpage

\section*{Problem 4}
\textbf{Main Idea:}\\
Observe that $V_i$ is completely irrelevant to this problem; since all tasks will be completed eventually, all $V_i$s will be recieved.
We want to do tasks that have a larger penalty earlier (complete larger $P_i$).
At the same time, we want to do tasks that can be completed quickly earlier.
Hence, we complete tasks with lower $\frac{R_i}{P_i}$ (or higher $\frac{P_i}{R_i}$) before tasks with higher $\frac{R_i}{P_i}$ (or lower $\frac{P_i}{R_i}$).
The algorithm we will use to achieve this is to sort tasks by $\frac{R_i}{P_i}$, and schedule each task one after another.\\
\textbf{Pseudocode:}\\
\begin{verbatim}
def makeSchedule(tasks):
    heap = new MinHeap()
    for task in tasks:
        heap.insert(value=task,key=task.R/task.P)
    schedule=[]
    while not isEmpty(heap):
        schedule.append(heap.removeMin())
    return schedule
\end{verbatim}
\textbf{Proof of Correctness:}\\
Our algorithm enforces the invariant that in a schedule $S = [(R_0,P_0,V_0), (R_1,P_1,V_1) \cdots (R_n,P_n,V_n)]$, $\frac{R_i}{P_i}<\frac{R_j}{P_j}\Leftrightarrow i<j$.
Suppose for contradiction that there exists some schedule T with a lower penalty than the schedule S produced by our algorithm.
In order to arrive at T from S,  it is necessary to rearrange elements.
Namely, for at least some i,j, where $(R_i,P_i,V_i)$ comes earlier in S than $(R_j,P_j,V_j)$, the positions of $(R_i,P_i,V_i)$ and $(R_j,P_j,V_j)$ are swapped in T.
Since $(R_i,P_i,V_i)$ comes before $(R_j,P_j,V_j)$ in S, $i<j$ in S, but $j<i$ in T.
But we know that $i<j \Rightarrow \frac{R_i}{P_i}<\frac{R_j}{P_j}$, so $(R_i)(P_j)<(R_j)(P_i)$.
From the problem statement, we know that the cost C of any schedule is given by $\sum_{i=0}^{n}((R_i)(\sum_{j=i}^{n}P_j))$.
Hence, since $i<j$ in S, there is a $(R_i)(P_j)$ term in S, but no $(R_j)(P_i)$ term. 
Likewise, we know that there is a $(R_j)(P_i)$ term in T, but no $(R_i)(P_j)$ term.
Since our algorithm enforces that $(R_i)(P_j)<(R_j)(P_i)$, this implies that T is less optimal than S.
However, remember that T represents any configuration in which any two elements in S are swapped.
Any deviation from S causes an increase in total penality.
Hence, S is the optimal schedule, and our algorithm is correct.
\\
\textbf{Running Time:}\\
$O(n log(n))$\\
\textbf{Running Time Analysis:}\\
The main operation involved in performing this algorithm is sorting the list of tasks by $\frac{R_i}{P_i}$, which takes $\boxed{O(n log(n))}$ time. 
In our case, the O(n log(n)) sorting operation is inserting all elements into the min heap, in a heapsort.
\label{pg:end-of-p4}

% Make sure that the solution here does not exceed one page here. If
% it does, use the extra space for this problem at the end.  
%
% Comment out the next line if you are NOT using the extra space
\paragraph{} \emph{Continued on Page \pageref{pg:p4-continuation}}


\newpage

\section*{Problem 5}
\textbf{Main Idea:}\\
Our algorithm is very similar to the set cover algorithm in the book except that we order by cost per node in the set.
Each set $s_i$ has a weight $w_i$. 
To find our solution, we want to order the set S of sets by $\frac{w_i}{|s_i|}$, that is, the weight of the set divided by the number of nodes included in the set.
The motivation behind this is that the aforthmentioned quanitity represents the cost per node in the set.
In our greedy solution, we want to add sets that cover a large number of nodes, at a low cost, so we reward large set size and low cost.
We then add sets from the ordered S in order of $\frac{w_i}{|s_i|}$ until all nodes are covered.\\
\textbf{Pseudocode:}\\
\begin{verbatim}
def weighted_set_cover(S):
    #input S:list of sets
    C = Empty_Set()
    outputSets = []
    orderedSets = MinHeap(S,key=lambda s: s.weight/len(s))
    while C != S:
        curr = orderedSets.removeMin()
        outputSets.append(curr);
        C=C.union(curr)
    return outputSets
\end{verbatim}
\textbf{Cost Analysis:}\\
We will show that this algorithm produces a result of cost $O(k log(n))$, where k is the optimal solution cost.
To do so, we must construct a worst case scenario.
In any set of N nodes, the worst case for this algorithm is a set of sets such that the amount of redundancy is maximized.
To construct this worst case, let $n_1$ be node 1, $n_2$ be node 2, et cetera. 
Construct N sets. Let set $s_i$ include all nodes $n_j, j \le i$. So Set 1 contains $n_1$, set 2 contains $n_1, n_2$, and set n contains all the nodes.
Suppose that the weights of each set were such that the algorithm adds all of the sets to the output. 
Given this, the most expensive set must be $s_n$, the set that contains all the nodes (Supppose that this were not the case, and $w_n<w_i$ for some $i<n$. Then $s_i$ would not be added to the solution because $\frac{w_i}{i}>\frac{w_n}{n}$).
The cost of the solution computed by the algorithm is $C=\frac{w_1}{1}+\frac{w_2}{2}+\cdots+\frac{w_n}{n}$.
But $w_n>w_{i\neq n}$.
So $C\le \frac{w_n}{1}+\frac{w_n}{2}+\cdots+\frac{w_n}{n}=(w_n)(\frac{1}{1}+\frac{1}{2}+\cdots+\frac{1}{n})$
But $\frac{1}{1}+\frac{1}{2}+\cdots+\frac{1}{n}$ is just the harmonic series, and is bounded by $ln(n)$.
So $C\le w_n ln(n)$.
Since $s_n$ covers the entire set, the optimal cost $k=w_n$.
So $C\le k \frac{log(n)}{log(e)}=\boxed{klog(n)}$.
\label{pg:end-of-p5}

%Insert solution here


% Make sure that the solution here does not exceed one page here. If
% it does, use the extra space for this problem at the end.  
%
% Comment out the next line if you are NOT using the extra space
\paragraph{} \emph{Continued on Page \pageref{pg:p5-continuation}}


\newpage


%% Comment out the "extra spaces" completely for the problems for you
%% don't need them

\section*{Extra space for Problem 1}
\emph{Continued from Page \pageref{pg:end-of-p1}}\\

%Insert solution here


\label{pg:p1-continuation}

\newpage
%%Comment out the above three lines if you are not using extra space
%%for this problem.


\section*{Extra space for Problem 2}
\emph{Continued from Page \pageref{pg:end-of-p2}}\\

%Insert solution here

\label{pg:p2-continuation}
\newpage
%%Comment out the above three lines if you are not using extra space
%%for this problem.


\section*{Extra space for Problem 3}
\label{pg:p3-continuation}
\emph{Continued from Page \pageref{pg:end-of-p3}}\\
%Insert solution here

\newpage
%%Comment out the above three lines if you are not using extra space
%%for this problem.



\section*{Extra space for Problem 4}
\emph{Continued from Page \pageref{pg:end-of-p4}}\\
\label{pg:p4-continuation}

\newpage
%%Comment out the above three lines if you are not using extra space
%%for this problem.



\section*{Extra space for Problem 5}
\emph{Continued from Page \pageref{pg:end-of-p5}}\\

\label{pg:p5-continuation}

\newpage
%%Comment out the above three lines if you are not using extra space
%%for this problem.



\end{document}
