What is Artificial Intelligence?
================================
	\*Artificial Intelligence\* gives intelligence to computers
	Machine Learning: Learning from statistical datasets

	Sentiment Analysis using a Naive Bayes Classifier
	-------------------------------------------------

		Probability
		-----------
			The \*sample space\* is a fancy term for all the possible things (outcomes) that could happen.

			A \*probability function\* tells you the probability that a certain event will happen.

			\*conditional probability\*: probability of event A, given B happened.

			Example:
				Event A:
					Ms. Pacman gets an A in 61B

				Event B:
					Prof. Hilfinger is teaching 61B

				P(A|B)=(P(A \intersect B))/(P(B))

		Classifier
		----------
			Define categoies or classes (eg. "spam", "important", "pizza spam")
			Pick observations or features that give us insight for the class.

			A \*classifier\* takes in features and give them a class.

		Bayesian Networks
		-----------------
			-Graphical representation of a probability model
			-The circles, or nodes, represent variables.
			-The arrow denotes that a variable influences another.

			-The probability of any variable is the product of its probabilities conditioned on all its parents.
			-For example: P(G)=P(G|S)*P(G|R)

			Naive Bayes Classifier
			----------------------
				-\*Naive Bayes classifier\*, using Beyesia networks
				-One root node, which is the class (eg. "spam or not")
				-Root node points to many child nodes, and features-
				Drawing:
					   --->x_1
					y--|-->x_2
					   --->x_3
				-y is "tweet is good or bad"
				-x_1...x_n are the presence of words like "lol" or "wtf"
				-What does naive mean?
					-"not good" interpreted as "good". Does not look at context.

			Naive Bayes Classifier -Inference
			---------------------------------
				So you have the values of features; how do you pick the class?
				Pick the class c with the highest probability given those features.
				
				P(C=c|F_1...F_n), probability of class c and features F_(1 ... n)
				P(C|F_1, ...,F_n)=(P(C,F_1,...,F_n))/(P(F_1,...,P_n)
				=> P(C|F_1, ...,F_n) \proportional (P(F_1|C)*...*P(F_n|C))

				"The concert was great."

				Multiply positive|true probabilities together with negative|false and compare with mulitplying negative|true probabilities together with positive|false.

				PROBLEM: When you have a lot of features, multiplying probabilities gives you very small numbers.
				Instead, use sum of log probabilities
				\sum log P(F|C)

			But how do we pick features?
			----------------------------
				Entropy
				-------
					\*Entropy\* is a measure of unpredictability of a set.
					\*Information gain\* tells us improvement o a set S by splitting it into k small subsets.

					Features are items that increase information gain and decrease Entropy

					H(S)=entropy=\sum_i p_i log(p_i)

					Infformation Gain
					Gain(S,S_1,...S_k)=H(S)-\sum_k (|S_k|)/(|S|) H S_k
