Caching
=======
  Hold small subset of data in cache, which is segment of memory that's easy to reach

  Principle of locality:
  ----------------------
	Programs access only a smal portion of full address space at any instant of time
	  Adddress space holds both code and data.
	
	Temporal Locality (locality in time)
	  Memory recently accessedis similar to be used again

	Spatial Locality (locality in space)
	  Related memory is likely to be located close together.
	
	Memory closest to processor is faster to access.
	  Closer is smaller, faster, expensive
	  Farther is larger, slower, cheaper
	
  Caches use static RAM (SRAM)
  SRAM is fast and expensive
  Main memory use Dynamic RAM (DRAM)
  DRAM is slow but cheap

  The block is the unit of transfer between memory and cache.

  Goal: Present the programer with about as much memory as the largest memory and about the speed of the fastest memory.

Fully Assocative Caches
-----------------------
  Cache is "cold" when you start up. Contains garbage. Encoded with a \*valid bit\*.
  Block contains K adjacent bytes.

  Effect of Block Size
	Offset Field: Lowest bits of memory address can be used to index specific bytes within a block.
	Block size needs to be a power of 2 (in bytes)
	(address) modulo # of bytes in a block

  Effect of cache size (C Bytes)
	Cache size refers to total stored data
	Determines numberof blocks the cache can hold.
	Tag Field: Leftover upper bits of memory address determine the portion fo memory the block came from (identifier)

  Every memory block can map anywhere in cache (Fully Assocative).
	Most efficient use of space
	Least efficient to check

  To check a fully associative cache:
	Look at all cache slots in parallel
	If valid bit is 0, ignore
	If valid bit is 1 and tag matches, return that data

  To address something in cache, have Tbits Tag bits + OBits Offset bits

  - O Bits <-> 2^O bytes/block = 2^(O-2) words/block
  - T Bits = A-O where A = # address bits

  Caching terminaology
	When reading memory, 3 things can hapen:
	  Cache hit:
		Cache holds valid copy of the block, so return the desired data
	  Cache miss:
		Cache does not contian desired block, so fetch from memory and put in empty slot
	  Cache miss with block replacement:
		Cache does not have desired block and is full, so discard one and replace it with desired data
	  Block  Replacement Policies
		Random Replacement
		Least Recently Used (requres log_2(C/K) management bits)
	  Cache effectiveness:
		Hit Rate: Percentage of hits
		Miss Rate: 1-HR
	  Cache speed:
		Hit time: Time to access cache
		Miss penalty: Time to replace a block in cache from lower level in memory heirarchy

  Two seperate caches for instruction and data (I$, D$)

  Handling Cache Hits
	Read Hits (I$ and D$)
	  Read from cache.
	Write hits (D$)
	  Write-through policy: Always write data to cache and memory (through cache)
		Forces cache and memory to always be consistentoInclude a Write buffer to write to memory in parallel
	  Write-back policy: Write data only to cache, then update memory when block removes
		Allows memory and cache to be inconsistent
		Multiple writes collected in cache; single write to memory per block
		Use Dirty Bit--Extra bit per cache row that is set if block was written to (is dirty) and needs to be written back
  Miss penality grows with block size
	Read misses
	  Read from memory, store in cache.
	Write misses
	  Write allocate: Fetch block, put in cache, execute write hit
		Works with either write-through or write-back
		Ensures cache is up-to-date after write miss
	  No-write allocate:
		Skip cache altogether and write directly to memory
		Ensures memory always up to date
